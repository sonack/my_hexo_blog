---
title: 机器学习总结
pid: 机器学习总结
categories:
  - ML
tags: [李宏毅,note,machine learning]
date: 2017-05-30 12:00:35
---

<center>李宏毅课程总结，课程名称为NTUEE ML 2016。</center>

<!-- more -->
### Learning Map

![](http://ww1.sinaimg.cn/large/006cbtZIly1fg39z88b3uj30gv0cyjsh.jpg)

### Regression

3 steps:


**Step1**
Model: A set of function
**Step2**
Goodness of Function: min Loss function L(f)
**Step3**
Best Function: Gradient Descent/Ascent



#### where does the error come from?

error on testing data = bias + variance

### Gradient Descent

梯度：总是Loss函数的等高线的法线方向

#### Tip1. Tuning Your Learning Rate

Adagrad:

![](http://ww1.sinaimg.cn/large/006cbtZIly1fg3aqk3e4aj30el032aa6.jpg)

#### Tip2. Stochastic Gradient Descent

pick an example xn
only consider loss for only one example

update times is as many as example numbers

fast!!

#### Tip3. Feature Scaling


make different features have the same scaling

![](http://ww1.sinaimg.cn/large/006cbtZIly1fg3avot7cdj30fe0583yv.jpg)

#### Limitations:

![](http://ww1.sinaimg.cn/large/006cbtZIly1fg3b27ixu7j30fd09676c.jpg)


### Classification

#### Probabilistic Generative Model


generative model:

P(x) = P(x|C1)P(C1) + P(x|C2)P(C2) + ....


假设数据从一个Gaussian Distribution Sample而来，其由mean μ和covariance matrix Σ决定。请参考[https://blog.slinuxer.com/tag/pca](https://blog.slinuxer.com/tag/pca)。


##### Maximum Likelihood


if you assume all the dimensions independent,then you are using Naive Bayes Classifier.如果假设每个属性都服从1D-Gaussion，相当于covariance matrix变成对角矩阵(只有对角线有非零元素)。



后验概率(posterior probability)


### Logistic Regression

#### Why Cross Entropy Not Square Error?

![](http://ww1.sinaimg.cn/large/006cbtZIgy1fg3cqikc38j30an065jse.jpg)


#### Multiclass Classification

soft-max，Loss Function仍是Cross Entropy。



### Deep Learning

#### Brief Introduction

cascading logistic regression

![](http://ww1.sinaimg.cn/large/006cbtZIgy1fg3d4h3yvzj30pf0f3408.jpg)

**Universality Theorem**
[http://neuralnetworksanddeeplearning.com/chap4.html](http://neuralnetworksanddeeplearning.com/chap4.html)

### Backpropagation

pass

### Tips for Deep Learning


#### 在训练数据上就表现不好


##### 新的激活函数
**ReLU**

解决vanishing gradient problem。

rectified linear unit

![](http://ww1.sinaimg.cn/large/006cbtZIgy1fg3eqixitrj305l04qglf.jpg)



reason:
1. Fast to compute
2. Biological reason
3. 可以看做无限多个不同biases的sigmoid函数的结合
4. 可以解决梯度弥散问题

ReLU: a thinner linear network 

**ReLU的variant**

![](http://ww1.sinaimg.cn/large/0060lm7Tgy1firawrr9yij30b005bgln.jpg)
α也是由GD学习得来


**Maxout**

ReLu is a special case of Maxout

Learnable activation function


##### Adaptive Learning Rate


**RMSProp**

Adagrad中用一阶导数的root mean sqaure来逼近二阶导数的方式

![](http://ww2.sinaimg.cn/large/0060lm7Tgy1firb6xwh2xj30ay035jre.jpg)

此处用指数移动加权平均

![](http://ww3.sinaimg.cn/large/0060lm7Tgy1firb7w2lkcj307u01ft8j.jpg)

**Momentum惯性**

movement仍然是之前各次gradients的线性组合。

v2 = λv1 - η▽L(θ1)

**Adam**

Adam = RMSProp + Momentum

---

#### 在测试数据上表现不好(过拟合)

在训练数据上表现的好，但是在测试数据上表现不好 ==> Overfitting!

##### Early stopping

利用validation set来替代testing set

##### Regularization

Find a set of weight not only minimizing original cost but also close to zero

==> new Loss Function

L' = L(θ) + λ(1/2)|θ|^2

通常不考虑biases,|θ|^2 = (w1)^2 + (w2)^2 + ... 

Gradient:

![](http://ww1.sinaimg.cn/large/0060lm7Tgy1firbg15lruj303y01iglf.jpg)

Update: Wt+1 = (1-ηλ)Wt - ηdL/dW  == **weight decay**

L1 regularization:

![](http://ww4.sinaimg.cn/large/0060lm7Tgy1firbinua9tj30cf06caak.jpg
)


##### Dropout

Each time before updating the parameters, each neuron has p% to dropout. the structure of the network become more thinner, and we using the new network for trainning.

for each mini-batch, we resample the dropout neurons.

When testing, no dropout, but if the dropout rate at trainning is p%, then the weights times (1-p%).


dropout is a kind of ensemble.

train a bunch of networks with different structures.

using one mini-batch to train one network;
some parameters in the network are shared.



### Convolutional Neuron Network


#### Why CNN for Image?

1. Some pattern are much smaller than the whole image
2. The same patterns appear in different regions
3. subsampling the pixels will not change the object

#### The whole CNN

Property1、2: Convolution

Property3: MaxPooling 

**Convolution**

stride

Feature Map

each filter is a channel

#### What does CNN learn?

degree of the activation of the k-th filter:

![](https://latex.codecogs.com/gif.latex?a%5Ek%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7B11%7D%5Csum_%7Bj%3D1%7D%5E%7B11%7D%20a_%7Bi%2Cj%7D%5Ek)

利用gradient ascent找出

![](https://latex.codecogs.com/gif.latex?%5Cbg_white%20x%5E*%3D%5Carg%20%5Cmax_x%20a%5Ek)

Deep Neural Networks are Easily Fooled

Deep Dream: CNN exaggerates what it sees.

Deep Style: content + style

Playing Go: alpha Go does not use Max Pooling...

Speech: CNN's filter move in the frequency direction on the spectrogram.

Text: time direction

How to let machine draw an image?

* PixelRNN
* VAE(Variation Autoencoder) 
* GAN(Generative Adversarial Network)

### Why Deep?

Not surprised, more parameters, better performance.

Fat + Short v.s. Thin + tall

Modulariztion（模块化）

Deep means modularization

the basic classifiers are shared by the following classifiers as modules.

modularization requires less trainning data. 

the modularization is automatically learnt from data.

**Modularization-Speech**

the hierarchical structure of human languages

phoneme: 音素，发音要素

phoneme受到前后phoneme的影响，因此利用tri-phone来表示。

每个phoneme有若干个state.

The first stage of speech recognition:

classification:
input -> acoustic feature
output -> state






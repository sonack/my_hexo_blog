---
title: 机器学习总结
pid: 机器学习总结
categories:
  - ML
tags: [李宏毅,note,machine learning]
date: 2017-05-30 12:00:35
---

<center>李宏毅课程总结，课程名称为NTUEE ML 2016。</center>

<!-- more -->
### Learning Map

![](http://ww1.sinaimg.cn/large/006cbtZIly1fg39z88b3uj30gv0cyjsh.jpg)

### Regression

3 steps:


**Step1**
Model: A set of function
**Step2**
Goodness of Function: min Loss function L(f)
**Step3**
Best Function: Gradient Descent/Ascent



#### where does the error come from?

error on testing data = bias + variance

### Gradient Descent

梯度：总是Loss函数的等高线的法线方向

#### Tip1. Tuning Your Learning Rate

Adagrad:

![](http://ww1.sinaimg.cn/large/006cbtZIly1fg3aqk3e4aj30el032aa6.jpg)

#### Tip2. Stochastic Gradient Descent

pick an example xn
only consider loss for only one example

update times is as many as example numbers

fast!!

#### Tip3. Feature Scaling


make different features have the same scaling

![](http://ww1.sinaimg.cn/large/006cbtZIly1fg3avot7cdj30fe0583yv.jpg)

#### Limitations:

![](http://ww1.sinaimg.cn/large/006cbtZIly1fg3b27ixu7j30fd09676c.jpg)


### Classification

#### Probabilistic Generative Model


generative model:

P(x) = P(x|C1)P(C1) + P(x|C2)P(C2) + ....


假设数据从一个Gaussian Distribution Sample而来，其由mean μ和covariance matrix Σ决定。请参考[https://blog.slinuxer.com/tag/pca](https://blog.slinuxer.com/tag/pca)。


##### Maximum Likelihood


if you assume all the dimensions independent,then you are using Naive Bayes Classifier.如果假设每个属性都服从1D-Gaussion，相当于covariance matrix变成对角矩阵(只有对角线有非零元素)。



后验概率(posterior probability)


### Logistic Regression

#### Why Cross Entropy Not Square Error?

![](http://ww1.sinaimg.cn/large/006cbtZIgy1fg3cqikc38j30an065jse.jpg)


#### Multiclass Classification

soft-max，Loss Function仍是Cross Entropy。



### Deep Learning

#### Brief Introduction

cascading logistic regression

![](http://ww1.sinaimg.cn/large/006cbtZIgy1fg3d4h3yvzj30pf0f3408.jpg)

**Universality Theorem**
[http://neuralnetworksanddeeplearning.com/chap4.html](http://neuralnetworksanddeeplearning.com/chap4.html)

#### Backpropagation

pass

#### Tips for Deep Learning

##### ReLU

解决vanishing gradient problem。

rectified linear unit

![](http://ww1.sinaimg.cn/large/006cbtZIgy1fg3eqixitrj305l04qglf.jpg)


### Convolutional Neuron Network


#### Why CNN for Image?

1. Some pattern are much smaller than the whole image
2. The same patterns appear in different regions
3. subsampling the pixels will not change the object

#### The whole CNN




